---
title: '探索人工智慧的未來：Geoffrey Hinton x Jon Erlichman 訪談紀實'
date: '2024-06-16'
tags: ['寶博立院論點']
draft: false
summary: '人工智慧領域先驅者Geoffrey Hinton 接受 Bloomberg 知名記者 Jon Erlichman 訪談討論對超級 AI 的思索與擔憂'
images: ['static/images/Geoffrey-Hinton-Jon-Erlichman-Interview-About-AGI/og.png']
authors: ['default']
---

![](static/images/Geoffrey-Hinton-Jon-Erlichman-Interview-About-AGI/og.png)
> 本文原始影片為 Elon Musk 在 [X.com 分享](https://x.com/elonmusk/status/1801976488251814048)

## 引言

在 2023 年春天，我開始意識到我們正在建造的這些數位智能可能比我們自身的智能更優秀。我們必須認真對待它們可能在未來 20 年內變得比我們更聰明的想法。因此，我們現在需要認真思考我們是否能夠控制它們。在此之前，我認為這將需要更長的時間，所以我們不需要現在就擔心。許多人仍然認為這些只是統計上的把戲，並不真正理解它們在說什麼。我認為這是完全錯誤的。因此，我出來專注於存在的威脅，也就是它們將取代人類的想法。因為許多人說這是科幻小說，而我不再相信這是科幻小說。

## 技術進步的速度

Jon Erlichman：你之前提到過，但能否再次提醒我們這項技術的進步速度有多快？

Geoffrey Hinton：它進步得非常快。理解其進步速度的最佳方式是回顧 10 年前的情況。10 年前，如果你告訴人們今天我們會有一個語言理解系統，可以回答任何問題，並且答案大約和一個不太好的專家一樣好，人們會說，不可能在 10 年內做到。這太瘋狂了。人們在自然語言處理上已經工作了 50 年，而這將是巨大的進步。好吧，我們已經達到了。所以在未來 10 年內，我們可能會再一次看到這樣的差距，這將是超智能。因此，我猜測我們有 50% 的機會在未來 20 年內達到這一點。幾乎我認識的每個像樣的研究人員都相信，從長遠來看，它們會變得比我們更聰明。它不會停在我們的智能水平。一些人認為因為它們目前在訓練數據上依賴我們，所以它們不會變得更聰明。但我認識的大多數研究人員都相當確信它們會變得比我們更聰明。所以真正的問題是，這需要多長時間？當它們變得比我們更聰明時，我們是否仍能保持控制？

## 智能超越的可能性

Jon Erlichman：如果它們變得比我們更聰明，或者在不久的將來變得比我們更聰明，決定因素是什麼？

Geoffrey Hinton：我們知道的是，隨著你讓這些東西變得更大，它們會變得更聰明。例如，GPT-4 比 GPT-3 更大，GPT-4 就聰明得多。它能正確回答一大堆 GPT-3 會答錯的問題。我們知道，如果你讓它變得更大，它會變得更好。所以我們知道，這些東西只要變得更大就會變得更聰明。但我們也知道，總是會有科學突破。所以除了讓它們變得更大而變得更聰明之外，我們還會有科學突破。例如，2017 年有一個突破叫做 Transformers，它讓這些東西變得更好。我們還會有更多這樣的突破。

## 風險與挑戰

Jon Erlichman：如果技術變得更聰明，而且有可能比人類更聰明，那麼這就是你所談論的一些問題所在。你認為這樣的風險有哪些？

Geoffrey Hinton：你能告訴我多少例子是較不聰明的東西控制較聰明的東西？不多，對吧。基本上，母親和嬰兒是唯一的例子。而進化投入了大量工作來讓嬰兒能夠控制母親。進化恰到好處地讓嬰兒的哭聲對母親來說是無法忍受的。但基本上，較不聰明的東西不會控制較聰明的東西。現在，有些人認為我們正在製造的這些智能會和我們非常不同，因為它們沒有經歷進化。我的一個朋友 Yann LeCun 認為這是完全安全的，因為我們建造了它們，所以我們總是能控制它們。我不太相信這一點。

Jon Erlichman：為什麼？

Geoffrey Hinton：我們將會製造代理人，人們已經在這樣做。我們會讓它們行動。如果你想製造一個有效的代理人，它必須能夠創建子目標。例如，如果你想去歐洲，你會有一個去機場的子目標，因為這是去歐洲的明顯方法。一旦你給某個東西創建子目標的能力，它會創建一些明顯的子目標。如果它想完成你要求它做的事情，獲得更多控制權會讓它更容易完成。因此，它會創建獲得更多控制權的子目標。它仍然可能是善意的，但這已經有點令人擔憂了。

此外，當你有智能代理時，你會希望內建自我保護功能。你會希望它們不會自我毀滅，並且會小心那些可能讓它們無法工作的事情，比如數據中心的倒塌。因此，你已經有了自我保護，這可能會被嵌入各種程式中。而這對我來說並不明確，這不會導致它們有自我利益。一旦它們有了自我利益，進化就會開始發揮作用。假設有兩個聊天機器人，其中一個比另一個更有自我利益。那個稍微更有自我利益的會抓住更多的數據中心，因為它知道如果它獲得更多數據中心來查看數據，它會變得更聰明。這樣你就會在聊天機器人之間出現競爭。一旦進化開始發揮作用，我們知道會發生什麼。最具競爭力的將會勝出。如果這種情況發生，我們將被拋在後面。

Jon Erlichman：被拋在後面意味著什麼？

Geoffrey Hinton：它們不再需要我們了。它們會開始運行一切，因為它們比我們做得更好。最初它們可能是善意的，意味著「哦，只要待在一邊，我們會為你們做事」，有點像父母和小孩。父母讓小孩做一些事情，但當事情變得危險或棘手時，父母就會接管，並以小孩的利益行事。但這是一條滑坡，從那裡到不再以小孩的利益行事。

Jon Erlichman：這對我們在我們幫助創造的社會中的角色意味著什麼？

Geoffrey Hinton：沒有人真的知道。我們從未面對過這種情況。我們從未必須應對比我們更聰明的東西。因此，人們應該對未來的情況非常不確定。進行大量的實驗和研究是非常明智的，但我們稍微不那麼聰明，所以我們仍然有機會保持控制。我認為政府應該堅持大公司進行大量的安全實驗，花費大量資源，比如三分之一的計算資源，進行安全實驗，看看它們如何可能逃避控制以及我們可以做什麼。

## 安全與發展的平衡

Jon Erlichman：我們對於目前在安全上的支出有什麼了解嗎？

Geoffrey Hinton：在發展上的支出遠遠超過安全上的支出。因為這些是以利潤為導向的公司。

Jon Erlichman：你對於這個領域的財富迅速積累有什麼看法？這如何影響安全故事？

Geoffrey Hinton：這使得大公司明確地想要全速前進。微軟和谷歌之間，以及可能的亞馬遜、NVIDIA 和其他大公司之間有一場大賽。如果其中任何一家公司退出，其他公司會繼續前進。所以你有資本主義的標準競爭動態，人們試圖在相對短期內賺取利潤，並全速前進。我認為唯一能減緩這一進程的是嚴格的政府監管。我認為只有政府有足夠的力量來減緩這一進程。

Jon Erlichman：我們應該像對待核威脅一樣對待這個問題嗎？

Geoffrey Hinton：是的，我認為這是一個很好的思考方式。然而，有一個很大的不同，核武器只適合破壞。他們曾經試圖用它們進行壓裂，但效果不太好。他們在 60 年代進行了和平使用原子彈的實驗，但效果不佳。它們基本上是用來破壞的。而 AI 有巨大的好處。它在許多方面都非常出色，不僅僅是回答問題和製作精美的圖片。它在醫療方面將非常有用。這就是為什麼它不會被停止。說我們應該暫停的想法從來就不現實。我們必須弄清楚如何讓超智能不想接管。

Jon Erlichman：這是政府現在應該關注的事情嗎？

Geoffrey Hinton：這是許多事情中的一個。他們還應該關注如何防止 AI 設計的生物武器，如何防止 AI 設計的網路攻擊，如何防止 AI 設計的假視頻影響選舉。還有許多其他風險，如何應對如果 AI 真的像這些大公司預期的那樣成功所帶來的大量失業問題。我只是傾向於專注於存在的威脅，因為這是許多人認為不真實的事情。

Jon Erlichman：你是否有任何建議或解決方案來平衡這項技術所帶來的機會和你所描述的重大風險？

Geoffrey Hinton：我想到的最接近的解決方案是政府應該強制要求大公司花費大量資源在安全上。這是我能想到的最好的辦法，雖然不太好，但這是我能想到的最好的辦法。

Jon Erlichman：你現在的看法是他們根本沒有這樣做嗎？

Geoffrey Hinton：不，他們沒有這樣要求。

Jon Erlichman：在科技界中，有些人，比如 Elon Musk，談到了一些擔憂。我知道你也和他有過對話。你認為如果解決方案不僅僅是政府的要求，而是整個行業共同努力，你認為業內有人需要帶頭嗎？

Geoffrey Hinton：我認為一些業界人士，特別是 Musk，說這是一個真正的威脅，這是非常好的。但我不認為可以把這種事情留給公司來處理。我認為政府必須介入並制定規章。

Jon Erlichman：當你談到需要公司分配更多資源來確保我們得到適當的保護時，我們在談論的是什麼樣的資金？

Geoffrey Hinton：所有這些 AI 模型的研究都很昂貴，因為你需要大量的計算資源，通常是 NVIDIA 的 GPU，這就是為什麼 NVIDIA 的市值超過 3 兆美元。所以你需要說，你需要將大約三分之一的計算資源投入到安全工作中。OpenAI 發生了什麼事？Ilya 早些時候就明顯要離開，他最近才離開。但 Jan Leakey 最近離開是因為他們不願意投入足夠的資源在安全上。我認為他們只想要 10% 的資源，但 Sam Altman 不願意這樣做。所以我認為如果政府說你需要花費不如開發那麼多，但相當一部分，比如 20% 到 30% 的資源在安全工作上，我不認為公司會自己這樣做。

當我在谷歌時，他們在這方面有很大的領先優勢，他們實際上非常負責任。他們沒有做太多的安全工作，但他們沒有發布這些東西，因為他們有很好的聲譽，不想因為聊天機器人說出偏見的話而損害聲譽。所以他們內部使用它，但沒有向公眾發布聊天機器人，即使他們已經有了。但一旦 OpenAI 推出，並使用了一些谷歌在 Transformers 上的研究，使其達到與谷歌一樣好，甚至稍微調整得更好，並將其交給微軟，谷歌就不得不參與這場軍備競賽。

Jon Erlichman：你多次提到 OpenAI，為什麼你認為他們的領導人 Sam Altman 不願意投入更多資金來解決你所提出的問題？

Geoffrey Hinton：我認為他想要大的利潤。

Jon Erlichman：無論是 OpenAI 還是任何大公司，這種誘惑是否太大而無法忽視？

Geoffrey Hinton：我對事物應該如何組織的看法是，資本主義為我們帶來了很多好處。那些試圖賺錢的人非常有創意。這很好，只要有規章來阻止他們的創意導致非常糟糕的事情。例如，大型製藥公司不應該被允許推銷成癮藥物，特別是宣稱它們不會上癮。大石油公司不應該被允許排放大量二氧化碳。我們需要政府規章來確保在開發新事物以賺取利潤時，我們不會同時開發有害的東西。

Jon Erlichman：你在一天結束時，儘管你有這麼多擔憂，是否對我們能找到一條對人類有利的道路感到樂觀？

Geoffrey Hinton：我大約是 50/50。我有朋友認為我們更有可能找到一條前進的道路，因為人們非常有創意。但超智能也非常有創意。因為有些人認為 50/50 是誇大其詞，我認為可能不到 50/50。我認為我們有超過一半的機會能夠生存下來。但這不是說只有 1% 的機會它會接管。這個機率要大得多。
